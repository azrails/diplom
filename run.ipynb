{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/azrails/diplom\n",
    "%cd diplom\n",
    "%pip install requirements.txt\n",
    "# download\n",
    "!mkdir datasets \n",
    "%cd datasets\n",
    "!wget http://images.cocodataset.org/zips/train2014.zip\n",
    "\n",
    "# unzip\n",
    "!unzip train2014.zip -d images/ && rm train2014.zip\n",
    "# download\n",
    "!wget https://bvisionweb1.cs.unc.edu/licheng/referit/data/refcoco.zip\n",
    "# unzip\n",
    "!unzip refcoco.zip && rm refcoco.zip\n",
    "\n",
    "!python ../data_utils/prepare_data.py --data_root . --output_dir . --dataset refcoco --generate_mask\n",
    "!python ../data_utils/folder_to_lmdb.py -j anns/refcoco/train.json -i images/train2014 -m masks/refcoco/ -o lmdb/refcoco\n",
    "!python ../data_utils/folder_to_lmdb.py -j anns/refcoco/val.json -i images/train2014/ -m masks/refcoco -o lmdb/refcoco\n",
    "!python ../data_utils/folder_to_lmdb.py -j anns/refcoco/testA.json -i images/train2014/ -m masks/refcoco -o lmdb/refcoco\n",
    "!python ../data_utils/folder_to_lmdb.py -j anns/refcoco/testB.json -i images/train2014/ -m masks/refcoco -o lmdb/refcoco\n",
    "\n",
    "# clean\n",
    "!rm -rf refcoco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "BASE_DIR = '/content/gdrive/MyDrive/stage_one'\n",
    "os.makedirs(BASE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/gdrive/MyDrive/stage_one/tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import checkpoints, config\n",
    "from data_utils import tokenizer, dataset\n",
    "from model import vit, bert\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#sets random\n",
    "random_seed=42\n",
    "random.seed(42)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "device=\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\" )\n",
    "\n",
    "match device:\n",
    "    case \"cuda\":\n",
    "        torch.cuda.manual_seed_all(random_seed)\n",
    "    case \"mps\":\n",
    "        torch.mps.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = config.load_data(\"configs/stage_one.yaml\")\n",
    "model = vit.StageOneEncoder(**conf['model']['VITEncoder'])\n",
    "optimizer = torch.optim.AdamW(model.state_dict(),  **conf['optimizer_params'])\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer, \n",
    "            milestones=conf['train_settings']['milestones'], \n",
    "            gamma=conf['train_settings']['lr_decay']\n",
    "        )\n",
    "losses = []\n",
    "val_scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, scheduler, conf, losses, val_scores = checkpoints.load_checkpoint(BASE_DIR, \"checkpoint_name\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_model = bert.BertEmbedding(conf['model']['text_backbone'])\n",
    "tokenizer = tokenizer.get_bert_tokenizer(conf['model']['text_backbone'])\n",
    "train_dataset = dataset.ReferenceDataset(\n",
    "    **conf['data']['train'],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "train_data = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=conf['train_settings']['batch_size'],\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    num_workers=8\n",
    ")\n",
    "val_dataset = dataset.ReferenceDataset(\n",
    "    **conf['data']['val'],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "val_data = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=conf['train_settings']['batch_size'],\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    num_workers=8\n",
    ")\n",
    "epoch = (conf['train_settings']['start_epoch'], conf['train_settings']['epochs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, text_model, optimizer, checkpoint_path, scheduler, device=\"cpu\", tb_path=None):\n",
    "        self.model = model.to(device)\n",
    "        self.text_model = text_model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.tb_path = tb_path\n",
    "        self.checkpoint_path\n",
    "        self.loss_fn = torch.nn.TripletMarginLoss()\n",
    "        if self.tb_path is not None:\n",
    "            self.writer =  SummaryWriter(self.checkpoint_path, self.tb_path)\n",
    "    \n",
    "    def train(self, train_data, val_data, epochs, losses, val_scores, checkpoint_step=5, scheduler_step=1):\n",
    "        for epoch in tqdm(range(epochs[0], epochs[1]), desc='Epochs'):\n",
    "            train_loss = self.train_epoch(train_data)\n",
    "            val_acc = self.validate(val_data)\n",
    "            if self.tb_path is not None:\n",
    "                self.write.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "                self.write.add_scalar(\"MSE/val\", val_acc, epoch)\n",
    "            losses.append(train_loss)\n",
    "            val_scores.append(val_acc)\n",
    "            if epoch % scheduler_step == 0:\n",
    "                self.scheduler.step()\n",
    "            if epoch % checkpoint_step == 0:\n",
    "                conf['train_settings']['start_epoch'] = epoch + 1\n",
    "                checkpoints.save_checkpoint(\n",
    "                    self.checkpoint_path, \n",
    "                    f'epoch_{epoch}', \n",
    "                    conf, \n",
    "                    self.model, \n",
    "                    self.optimizer, \n",
    "                    self.scheduler, \n",
    "                    losses, \n",
    "                    val_scores\n",
    "                    )\n",
    "\n",
    "    def train_epoch(self, train_data):\n",
    "        self.model.train()\n",
    "        loss = 0\n",
    "        for _, (_, mask_batch, negative_batch, sentence_batch, att_mask_batch) in enumerate(tqdm(train_data, desc=\"Training\", leave=False)):\n",
    "            mask_batch = mask_batch.to(self.device)\n",
    "            negative_batch = mask_batch.to(self.device)\n",
    "            sentence_batch = sentence_batch.to(device)\n",
    "            att_mask_batch = att_mask_batch.to(device)\n",
    "            self.optimizer.zero_grad()\n",
    "            positive_predictions = self.model(mask_batch)\n",
    "            negative_predictions = self.model(negative_batch)\n",
    "            anchor_predictions = self.text_model(sentence_batch, att_mask_batch)\n",
    "            step_loss = self.loss_fn(anchor_predictions, positive_predictions, negative_predictions)\n",
    "            step_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            loss += step_loss.cpu().detach().item() * len(mask_batch)\n",
    "        return loss / len(train_data.dataset)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self, val_data):\n",
    "        self.model.eval()\n",
    "        val_acc = 0\n",
    "        mse = torch.nn.MSELoss()\n",
    "        for _, (_, mask_batch, negative_batch, sentence_batch, att_mask_batch) in enumerate(tqdm(val_data, desc=\"Validating\", leave=False)):\n",
    "            mask_batch = mask_batch.to(self.device)\n",
    "            sentence_batch = sentence_batch.to(device)\n",
    "            att_mask_batch = att_mask_batch.to(device)\n",
    "            positive_predictions = self.model(mask_batch)\n",
    "            anchor_predictions = self.text_model(sentence_batch, att_mask_batch)\n",
    "            step_loss = mse(positive_predictions, anchor_predictions)\n",
    "            val_acc += step_loss.cpu().detach().item() * len(mask_batch)\n",
    "        return val_acc / len(val_data.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, text_model, optimizer, BASE_DIR, scheduler, device, 'tb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(train_data, val_data, epoch, losses, val_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
